
# Tri·ªÉn khai Kubernetes HA Cluster v·ªõi 3 Master Node v√† nhi·ªÅu Worker Node

**CNI:** Calico  
**Container Runtime:** containerd  
**H·ªá ƒëi·ªÅu h√†nh:** Ubuntu 22.04+

---

## 1Ô∏è‚É£ Chu·∫©n b·ªã tr∆∞·ªõc

**Y√™u c·∫ßu:**
- 3 Master Nodes v√† >=1 Worker Node
- C√≥ th·ªÉ SSH v·ªõi quy·ªÅn `root` ho·∫∑c `sudo`
- C√°c node k·∫øt n·ªëi m·∫°ng n·ªôi b·ªô v·ªõi nhau (k·∫øt n·ªëi b·∫±ng IP v√† FQDN/Hostname)
- Swap ƒë√£ t·∫Øt ho√†n to√†n
- ƒê·ªìng b·ªô th·ªùi gian (NTP)
- tr·ªè DNS record tke-k8s v·ªÅ ip master-01
```
rm -rf /etc/netplan/50*
mv /etc/netplan/99* /etc/netplan/k8s.yaml
sudo systemctl restart systemd-resolved
sudo systemctl enable systemd-resolved
sudo ln -sf /var/run/systemd/resolve/resolv.conf /etc/resolv.conf
sudo netplan apply
sudo timedatectl set-timezone Asia/Ho_Chi_Minh
sudo apt install -y chrony
cat <<'EOF' | sudo tee /etc/chrony/chrony.conf
confdir /etc/chrony/conf.d
server 0.asia.pool.ntp.org iburst
sourcedir /run/chrony-dhcp
sourcedir /etc/chrony/sources.d
keyfile /etc/chrony/chrony.keys
driftfile /var/lib/chrony/chrony.drift
ntsdumpdir /var/lib/chrony
logdir /var/log/chrony
maxupdateskew 100.0
rtcsync
makestep 1 3
leapsectz right/UTC
EOF
sudo systemctl restart chrony
sudo systemctl enable chrony
sudo sed -i "2i 127.0.1.1 $HOSTNAME" /etc/hosts
sudo sed -i '3i ' /etc/hosts
sudo apt update -y && sudo apt upgrade -y
```

```
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

echo -e "overlay\nbr_netfilter" | sudo tee /etc/modules-load.d/containerd.conf
sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
sudo sysctl --system
```
---

## 2Ô∏è‚É£ C√†i containerd v√† Kubernetes packages tr√™n t·∫•t c·∫£ c√°c node

**Th·ª±c hi·ªán c√°c b∆∞·ªõc sau tr√™n t·ª´ng node (Master & Worker):**

```bash
wget https://github.com/containerd/containerd/releases/download/v2.1.4/containerd-2.1.4-linux-amd64.tar.gz -P /tmp/
sudo tar -C /usr/local -xzf /tmp/containerd-2.1.4-linux-amd64.tar.gz

sudo wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /etc/systemd/system/containerd.service
sudo systemctl daemon-reload
sudo systemctl enable --now containerd

wget https://github.com/opencontainers/runc/releases/download/v1.3.2/runc.amd64 -O /tmp/runc
sudo install -m 755 /tmp/runc /usr/local/sbin/runc

wget https://github.com/containernetworking/plugins/releases/download/v1.8.0/cni-plugins-linux-amd64-v1.8.0.tgz -P /tmp/
sudo mkdir -p /opt/cni/bin
sudo tar -C /opt/cni/bin -xzf /tmp/cni-plugins-linux-amd64-v1.8.0.tgz

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd

sudo apt install -y apt-transport-https ca-certificates curl gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

sudo reboot
```

---

## 3Ô∏è‚É£ Kh·ªüi t·∫°o Master node ƒë·∫ßu ti√™n

```bash
# B·∫°n c·∫ßn chu·∫©n b·ªã:
# - Control Plane Endpoint (VD: 10.10.10.11:6443)
# - Pod Network CIDR (VD: 192.168.0.0/16)
# - Service CIDR (VD: 10.96.0.0/12)
# - Danh s√°ch c√°c SAN (DNS ho·∫∑c IP)
```

```bash
sudo kubeadm init \
  --pod-network-cidr "<POD_CIDR>" \
  --control-plane-endpoint "<ENDPOINT:PORT>" \
  --apiserver-cert-extra-sans "<SAN1>,<SAN2>,..." \
  --upload-certs
```

> Example:

```shell
kubeadm init \
--pod-network-cidr 192.168.0.0/16 \
--service-cidr 10.96.0.0/12 \
--control-plane-endpoint k8s-tke.tanlv.io.vn:6443 \
--apiserver-cert-extra-sans k8s-tke.k8s.local \
--apiserver-cert-extra-sans k8s-tke.tanlv.io.vn \
--apiserver-cert-extra-sans 10.0.0.10 \
--apiserver-cert-extra-sans control-plane-01 \
--apiserver-cert-extra-sans control-plane-02 \
--apiserver-cert-extra-sans control-plane-03 \
--apiserver-cert-extra-sans 10.0.0.11 \
--apiserver-cert-extra-sans 10.0.0.12 \
--apiserver-cert-extra-sans 10.0.0.13 \
--apiserver-cert-extra-sans 61.14.236.252 \
--upload-certs \
--dry-run
```

```bash
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

---

## 4Ô∏è‚É£ C√†i Calico l√†m CNI plugin

```bash
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.0/manifests/tigera-operator.yaml
sleep 10
```
2 l·ªánh n√†y ch·∫°y c√°ch nhau 1 l√∫c

```
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.0/manifests/custom-resources.yaml
```

---

## 5Ô∏è‚É£ T·∫°o l·ªánh join v√† ch·ª©ng ch·ªâ

```bash
kubeadm token create --print-join-command > /tmp/node-join-cmd
cp /tmp/node-join-cmd /tmp/master-join-cmd

CERT_KEY=$(kubeadm init phase upload-certs --upload-certs 2>/dev/null | grep -E '^[a-f0-9]{64}$')
sed -i "s/\$/ --control-plane --certificate-key $CERT_KEY/" /tmp/master-join-cmd
```

```bash
mkdir -p /tmp/cluster-certs/etcd
chmod -R 777 /tmp/cluster-certs
cp /etc/kubernetes/pki/{ca.*,sa.*,front-proxy-ca.*} /tmp/cluster-certs/
cp /etc/kubernetes/pki/etcd/ca.* /tmp/cluster-certs/etcd
cp /tmp/master-join-cmd /tmp/node-join-cmd /tmp/cluster-certs/
```

```
sudo chown -R ubuntu:ubuntu /tmp/cluster-certs
```

---

## 6Ô∏è‚É£ Th√™m c√°c Master node c√≤n l·∫°i

**Tr√™n c√°c master node 2 v√† 3:**

```bash
scp -r -i id_rsa ubuntu@<MASTER1_IP>:/tmp/cluster-certs ~/
sudo mkdir -p /etc/kubernetes/pki/etcd/
sudo cp ~/cluster-certs/*.crt ~/cluster-certs/*.key ~/cluster-certs/*.pub /etc/kubernetes/pki/
sudo cp ~/cluster-certs/etcd/ca.* /etc/kubernetes/pki/etcd/
```

> Run join command for master node

---

## 7Ô∏è‚É£ Th√™m c√°c Worker node

**Tr√™n m·ªói worker node:**

> Run join command for worker node

tr·ªè l·∫°i DNS record tke-k8s v·ªÅ l·∫°i IP VIP
---

## 8Ô∏è‚É£ Ki·ªÉm tra Cluster

```bash
kubectl get nodes -o wide
kubectl get pods -A
```

## 9 C√†i INGRESS

deploy metallb:
```
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/refs/heads/main/config/manifests/metallb-native.yaml
```

```
vi metallb-config.yaml
```

```
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: metallb-pool
  namespace: metallb-system
spec:
  addresses:
  - 10.0.0.200-10.0.0.249  # D·∫£i IP ƒë∆∞·ª£c ch·ªçn
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2-advertisement
  namespace: metallb-system
spec: {}
```

```
kubectl apply -f metallb-config.yaml
```

cai Ingress Controller: dung helm

B1: caif helm
```
sudo snap install helm --classic
```
B2: Setup NGINX ingress by Helm
```
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace
```
## KI·ªÇM TRA INGRESS

```
root@master-01:~# kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
ingress-nginx-controller-645b679d5c-5vqr6   1/1     Running   0          3m51s
```

```
root@master-01:~# kubectl get ingressclass
NAME    CONTROLLER             PARAMETERS   AGE
nginx   k8s.io/ingress-nginx   <none>       3m15s
```

```
root@master-01:~# kubectl get svc -n ingress-nginx
NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.99.236.43   10.0.0.200    80:32491/TCP,443:31001/TCP   2m36s
ingress-nginx-controller-admission   ClusterIP      10.97.97.188   <none>        443/TCP                      2m36s
```

## 9 Mount NFS

```
 wget https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/refs/heads/master/deploy/install-driver.sh
```

```
chmod +x install-driver.sh
./install-driver.sh
```

ki·ªÉm tra

```
root@master-01:~# kubectl -n kube-system get pods -l app=csi-nfs-controller -o wide
NAME                                  READY   STATUS    RESTARTS       AGE     IP          NODE        NOMINATED NODE   READINESS GATES
csi-nfs-controller-6994f688bc-dszlv   5/5     Running   1 (7m4s ago)   7m44s   10.0.0.23   worker-03   <none>           <none>
root@master-01:~#
root@master-01:~#
root@master-01:~# kubectl -n kube-system get pods -l app=csi-nfs-node -o wide
NAME                 READY   STATUS    RESTARTS   AGE     IP          NODE        NOMINATED NODE   READINESS GATES
csi-nfs-node-7jc9d   3/3     Running   0          7m55s   10.0.0.22   worker-02   <none>           <none>
csi-nfs-node-8m8qv   3/3     Running   0          7m55s   10.0.0.12   master-02   <none>           <none>
csi-nfs-node-bwgvf   3/3     Running   0          7m55s   10.0.0.21   worker-01   <none>           <none>
csi-nfs-node-c7ff5   3/3     Running   0          7m55s   10.0.0.11   master-01   <none>           <none>
csi-nfs-node-m8964   3/3     Running   0          7m55s   10.0.0.13   master-03   <none>           <none>
csi-nfs-node-xc4nr   3/3     Running   0          7m55s   10.0.0.23   worker-03   <none>           <none>
```

```
vi sc-nfs.yaml
```

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-csi-sc
provisioner: nfs.csi.k8s.io
parameters:
  server: 10.0.0.70     # v√≠ d·ª•: 10.0.0.70
  share:  /data_k8s       # v√≠ d·ª•: /data_k8s
mountOptions:
  - nfsvers=4.1
  - rsize=1048576
  - wsize=1048576
  - hard
  - timeo=600
  - retrans=2
reclaimPolicy: Delete        # x√≥a PVC s·∫Ω x√≥a th∆∞ m·ª•c con tr√™n share
volumeBindingMode: Immediate
```

```
vi pvc-pod-test.yaml
```

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-csi-pvc
spec:
  accessModes: ["ReadWriteMany"]          # ƒëi·ªÉm m·∫°nh c·ªßa NFS
  storageClassName: nfs-csi-sc
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: nfs-csi-pod
spec:
  securityContext:
    fsGroup: 2000                         # gi√∫p ghi n·∫øu server d√πng root_squash
  containers:
  - name: app
    image: busybox:1.36
    command: ["sh","-c","id; touch /data/ok && echo hello > /data/hello.txt && sleep 3600"]
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: nfs-csi-pvc
```


```
kubectl apply -f sc-nfs.yaml
kubectl apply -f pvc-pod-test.yaml
```

```
root@master-01:~# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-41de37a4-86ec-41c0-99fb-bef2e1db5e2a   5Gi        RWX            Delete           Bound    default/nfs-csi-pvc   nfs-csi-sc     <unset>                          147m
root@master-01:~#
root@master-01:~#
root@master-01:~# kubectl get pvc
NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
nfs-csi-pvc   Bound    pvc-41de37a4-86ec-41c0-99fb-bef2e1db5e2a   5Gi        RWX            nfs-csi-sc     <unset>                 147m
root@master-01:~#
root@master-01:~#
root@master-01:~#
root@master-01:~# kubectl get pod
NAME          READY   STATUS    RESTARTS      AGE
nfs-csi-pod   1/1     Running   2 (27m ago)   148m
root@master-01:~#
```


# 10.Kubernetes Dashboard Deployment (Production Setup)

This guide describes how to deploy the **Kubernetes Dashboard** on a production-grade cluster using your **commercial SSL certificate** (not self-signed), and expose it at **https://eoh-k8s.tpcloud.vn**.

---

## 0) Prerequisites

- A running Kubernetes cluster with Ingress controller (NGINX preferred).
- A valid DNS record:
  - `eoh-k8s.tpcloud.vn ‚Üí 200.64.129.200`
- A commercial SSL certificate consisting of:
  - `fullchain.pem`
  - `privkey.pem`

---

## 1) Create Namespace & Secret for TLS

```bash
kubectl create namespace kubernetes-dashboard

kubectl -n kubernetes-dashboard create secret tls kubedash-tls   --cert=fullchain.pem   --key=privkey.pem
```

---

## 2) Install Kubernetes Dashboard using Helm

Add the official Helm repository and install the chart with custom values.

```bash
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm repo update

cat > values-dashboard.yaml <<'EOF'
app:
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - eoh-k8s.tpcloud.vn
    tls:
      enabled: true
      secretName: kubedash-tls
EOF

helm upgrade --install kubernetes-dashboard   kubernetes-dashboard/kubernetes-dashboard   --namespace kubernetes-dashboard   -f values-dashboard.yaml
```

> üõà Dashboard v7.x and above supports Helm installation only. The Ingress terminates TLS and routes traffic to the internal Dashboard service.

---

## 3) Create User Accounts and Tokens

### Viewer (Recommended for Daily Operations)

```bash
kubectl -n kubernetes-dashboard create serviceaccount viewer
kubectl create clusterrolebinding dashboard-view   --clusterrole=view   --serviceaccount=kubernetes-dashboard:viewer
kubectl -n kubernetes-dashboard create token viewer --duration=30m
```

### Admin (Temporary Full Access)

```bash
kubectl -n kubernetes-dashboard create serviceaccount admin-user
kubectl create clusterrolebinding admin-user-binding   --clusterrole=cluster-admin   --serviceaccount=kubernetes-dashboard:admin-user
kubectl -n kubernetes-dashboard create token admin-user --duration=10m
```

---

## 4) Verify Deployment

```bash
kubectl -n kubernetes-dashboard get pods -o wide
kubectl -n kubernetes-dashboard get svc,ingress -o wide
kubectl -n ingress-nginx get svc ingress-nginx-controller -o wide
```

Access the Dashboard via:
üëâ **https://eoh-k8s.tpcloud.vn**  
Sign in using the token generated above.

---

## 5) Security & Hardening Recommendations

- **Use RBAC minimum privilege:** Viewer for normal users, admin only when needed.
- **Short-lived tokens:** Always set `--duration` for token expiration.
- **Certificate chain:** Ensure your `fullchain.pem` includes intermediate CAs.
- **Ingress security:** Add security headers (HSTS, X-Frame-Options, etc.) at the Ingress Controller.

---

## 6) Troubleshooting

| Issue | Cause | Solution |
|-------|--------|-----------|
| 404 / ‚ÄúNo resources found‚Äù | Wrong ingress class or namespace | Verify `ingressClassName: nginx` and namespace `kubernetes-dashboard` |
| 502 / 504 Gateway Timeout | Pod not Ready | Check pod status with `kubectl get pods` |
| TLS Error | Missing intermediate cert | Use `fullchain.pem` with full chain |
| Invalid Token | Token expired | Recreate token with `kubectl create token ... --duration=...` |

---

## 7) Optional ‚Äì Install NGINX Ingress (If Missing)

```bash
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx   --namespace ingress-nginx --create-namespace
```

For bare-metal environments, expose via **MetalLB** or **NodePort**.

---

‚úÖ **You now have a fully functional, SSL-secured Kubernetes Dashboard accessible at:**
**https://eoh-k8s.tpcloud.vn**





